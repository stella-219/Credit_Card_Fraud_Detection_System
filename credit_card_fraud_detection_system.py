# -*- coding: utf-8 -*-
"""Credit_Card_Fraud_Detection_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hwgz57Jt0_n_kYd_7EQMQKtlax5vMn8q

**Read csv sample data into Pandas DataFrame**
"""
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

import numpy as np      # Provides support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays
import pandas as pd     # Used for data manipulation and analysis; provides functions to read and process structured data into DataFrames

# Libraries for visualization
import matplotlib.pyplot as plt     # A plotting library for creating static, animated, and interactive visualizations in Python
import seaborn as sns               # A data visualization library based on matplotlib that provides a high-level interface for drawing attractive statistical graphics

# Importing machine learning and data preprocessing libraries
from sklearn.model_selection import train_test_split     # Splits the dataset into training and testing subsets for model evaluation
from sklearn.linear_model import LogisticRegression      # Logistic regression classifier to predict the probability of a transaction being fraudulent or genuine
from sklearn.metrics import accuracy_score               # Calculates the accuracy of the model by comparing predictions to the actual values
from sklearn.metrics import classification_report        # Provides a detailed report on model performance, including precision, recall, F1 score, and support
from sklearn.preprocessing import RobustScaler           # Scales features to reduce the influence of outliers by scaling them within a range; ideal for highly skewed data

#Load credit card dataset to Pandas DataFrame
credit_card_data = pd.read_csv('Sample_Data/creditcard.csv')

"""**Data Visualization By Table & Plots**"""

#print the default top five rows and last five rows of the dataset
pd.concat([credit_card_data.head(), credit_card_data.tail()])

# Set up the figure and axis for the two plots
plt.figure(figsize=(6, 3))

# Plot the distribution of the 'Time' feature
plt.subplot(1, 2, 1)
sns.histplot(credit_card_data['Time'], kde=True, color='green')
plt.title('Distribution of Transaction Time')
plt.xlabel('Time (seconds)')
plt.ylabel('Frequency')

# Plot the distribution of the 'Amount' feature
plt.subplot(1, 2, 2)
sns.histplot(credit_card_data['Amount'], kde=True, color='blue')
plt.title('Distribution of Transaction Amount')
plt.xlabel('Amount')
plt.ylabel('Frequency')

# Display the plots
plt.tight_layout()
plt.show()
plt.close()  # Close the second plot

"""**Data Analysis To Understand the Fraudulent and Legit Transactions**"""

#Retrieve the DataSet information to check data completeness and understand data type of columns
#Alterative approach to check for data completness (eg. missing values) would be calling function isnull() then applying sum()
credit_card_data.info()

#Check the distribution of legit transactions(0) and fraudulent transactions(1)
#Note the result below shows this is highly unbalanced dataset

credit_card_data['Class'].value_counts()

"""Train Test Split"""

#Separate the legit and fraudulent transactions
legit = credit_card_data[credit_card_data.Class == 0]
fraud = credit_card_data[credit_card_data.Class == 1]

#Satistical measure of the legit and fraudulent transaction amounts

legit_stats = legit.Amount.describe()
fraud_stats = fraud.Amount.describe()

# Concatenate them into a single DataFrame
summary_table = pd.DataFrame({'Legit Transactions': legit_stats, 'Fraudulent Transactions': fraud_stats})

print(summary_table)

#Compare the values for both transactions
credit_card_data.groupby('Class').mean()

"""**Under - Sampling**

*   Build a sample dataset containing similar distribution of normal transactions

"""

#Random sampling of 492 legit transactions which is comparable to 492 fraudulent transactions
legit_sample = legit.sample(n=492)

#Concate two DataFrames
new_dataset = pd.concat([legit_sample, fraud], axis = 0)

#Print the default top 5 rows and last 5 rows of the new dataset
pd.concat([new_dataset.head(), new_dataset.tail()])

#Visualize the new dataset

plt.figure(figsize=(6, 3))

# Plot the distribution of the 'Time' feature
plt.subplot(1, 2, 1)
sns.histplot(new_dataset['Time'], kde=True, color='pink')
plt.title('Distribution of Transaction Time')
plt.xlabel('Time (seconds)')
plt.ylabel('Frequency')

# Plot the distribution of the 'Amount' feature
plt.subplot(1, 2, 2)
sns.histplot(new_dataset['Amount'], kde=True, color='yellow')
plt.title('Distribution of Transaction Amount')
plt.xlabel('Amount')
plt.ylabel('Frequency')

# Display the plots
plt.tight_layout()
plt.show()
plt.close()  # Close the second plot

#Check the value count
new_dataset['Class'].value_counts()

#Compare the values for both transactions from new dataset
new_dataset.groupby('Class').mean()

"""**Splitting the data into Features & Target**"""

#Split the date into two parts. One part for Class (last column) which will be used for Y and the remaining will be for X.
X = new_dataset.drop(columns = 'Class', axis = 1)
Y = new_dataset['Class']

print(X)

print(Y)

#Split the data into training data and test data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify = Y, random_state = 2)

#Print the number of test samples, traning samples and features
print(X.shape, X_train.shape, X_test.shape)

#Scale data for training the model
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""**Model Training**
*   Logistic Regression
"""

model = LogisticRegression()

#Training the logistic regression model with training data
model.fit(X_train_scaled, Y_train)

"""**Model Evluation**
*   Accuracy Score
"""

#Accuracy on training date
X_train_prediction = model.predict(X_train_scaled)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)
print('Accuracy on Training Data:', training_data_accuracy)

#Accuracy on test data
X_test_prediction = model.predict(X_test_scaled)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)
print('Accuracy on Test Data:',test_data_accuracy )

#Comprehensive evlauation which has the precision, recall and F1 score by class
print("Classification Report:\n", classification_report(X_test_prediction, Y_test))